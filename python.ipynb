{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682b1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf0dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DigitCaps(nn.Module):\n",
    "\n",
    "    def __init__(self, num_capsules=30,\n",
    "                 in_channels=20, out_channels=40):\n",
    "        '''Constructs an initial weight matrix, W, and sets class variables.\n",
    "           param num_capsules: number of capsules to create\n",
    "           param previous_layer_nodes: dimension of input capsule vector, default value = 1152\n",
    "           param in_channels: number of capsules in previous layer, default value = 8\n",
    "           param out_channels: dimensions of output capsule vector, default value = 16\n",
    "           '''\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        # setting class variables\n",
    "        self.num_capsules = num_capsules\n",
    "        self.out_channels = out_channels\n",
    "       \n",
    "        self.in_channels = in_channels # previous layer's number of capsules\n",
    "\n",
    "        # starting out with a randomly initialized weight matrix, W\n",
    "        # these will be the weights connecting the PrimaryCaps and DigitCaps layers\n",
    "        \n",
    "    def forward(self, u):\n",
    "        '''Defines the feedforward behavior.\n",
    "           param u: the input; vectors from the previous PrimaryCaps layer\n",
    "           return: a set of normalized, capsule output vectors\n",
    "\n",
    "           '''\n",
    "        previous_layer_nodes=u.size(1)   \n",
    "        self.W = nn.Parameter(torch.randn(self.num_capsules, previous_layer_nodes,\n",
    "                                          self.in_channels,self. out_channels))    \n",
    "        # adding batch_size dims and stacking all u vectors\n",
    "        u = u[None, :, :, None, :]\n",
    "        # 4D weight matrix\n",
    "        W = self.W[:, None, :, :, :]\n",
    "        print('U-shape->',u.shape)\n",
    "        print('W-Shape->',W.shape)\n",
    "        # calculating u_hat = W*u\n",
    "        u_hat = torch.matmul(u, W)\n",
    "\n",
    "        # getting the correct size of b_ij\n",
    "        # setting them all to 0, initially\n",
    "        b_ij = torch.zeros(*u_hat.size())\n",
    "\n",
    "        # moving b_ij to GPU, if available\n",
    "        if TRAIN_ON_GPU:\n",
    "            b_ij = b_ij.cuda()\n",
    "\n",
    "        # update coupling coefficients and calculate v_j\n",
    "        v_j = dynamic_routing(b_ij, u_hat, self.squash, routing_iterations=3)\n",
    "\n",
    "        return v_j # return final vector outputs\n",
    "\n",
    "\n",
    "    def squash(self, input_tensor):\n",
    "        '''Squashes an input Tensor so it has a magnitude between 0-1.\n",
    "           param input_tensor: a stack of capsule inputs, s_j\n",
    "           return: a stack of normalized, capsule output vectors, v_j\n",
    "           '''\n",
    "        # same squash function as before\n",
    "        squared_norm = (input_tensor ** 2).sum(dim=-1, keepdim=True)\n",
    "        scale = squared_norm / (1 + squared_norm) # normalization coeff\n",
    "        output_tensor = scale * input_tensor / torch.sqrt(squared_norm)\n",
    "        return output_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tensor = torch.rand(32, 34667, 10)\n",
    "digitCaps = DigitCaps()\n",
    "random_tensor = digitCaps(random_tensor)\n",
    "\n",
    "print(random_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
